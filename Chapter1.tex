\ifdefined\included
\else
\documentclass[english,a4paper,11pt,twoside]{StyleThese}
\include{formatAndDefs}
\sloppy
\begin{document}
\setcounter{chapter}{0} %% Numéro du chapitre précédent ;)
\dominitoc
\faketableofcontents
\fi

\chapter{From Human-Human Joint Action to Human-Robot Joint Action}
\minitoc

\section{Joint Action Theory}

A first step to endow robots with the ability to perform Joint Actions with humans is to understand how humans act together. As a working definition of Joint Action, we will use the one from \cite{sebanz2006joint}:

\begin{quote}
\textit{Joint action can be regarded as any form of social interaction whereby two or more individuals coordinate their actions in space and time to bring about a change in the environment.}
\end{quote}

A given number of prerequisites are needed for these individuals to achieve the so-called Joint Action. First of all, they need to agree on the change they want to bring in the environment, the conditions under which they will stay engaged in its realisation and the way to do it. A number of works have studied this prerequisite, named \textit{commitment}, which I will develop in Sec.~\ref{subsec:commitment}. Then, as mentioned in the definition, the individuals need to coordinate their actions in space and time. This will be studied in Sec.~\ref{subsec:coordination}. Finally, in order to coordinate, each individual needs to be aware of the other, he needs to be able to perceive him and predict his actions. This part will be develop in Sec.~\ref{subsec:prediction}.

\subsection{Commitment}

\label{subsec:commitment}

The first prerequisite to achieve a Joint Action is to have a \textit{goal} to pursue and the \textit{intention} to achieve it. Let's define in a first time what is called a \textit{goal} and an \textit{intention} for a single person before going to a \textit{joint goal} and a \textit{joint intention}.

In \cite{tomasello2005understanding}, Tomasello et al. define what they call a \textit{goal} and an \textit{intention} and illustrate these definitions with an example and an associated figure (fig.~\ref{fig:intention}) where a person wants to open a box.

\begin{figure}[!h]
	\centering
    \includegraphics[width=0.8\textwidth]{figs/Chapter1/intention.png}
    \caption{Illustrative example of an intentional action by Tomasello et al. Here the human has for \textit{goal} to open the box. He chooses a means to perform it and so forms an \textit{intention}.}
    \label{fig:intention}
\end{figure}

A \textit{goal} is defined here as the representation of the desired state by the agent (in the example, the goal is an open box) and, based on Bratman's work \cite{bratman1989intention}, an \textit{intention} is defined as an action plan the agent commits itself in pursuit of a goal (in the example, the intention is to use a key to open the box). The \textit{intention} includes both a \textit{goal} and the means to achieve it. 

In a same way, Cohen and Levesque propose in \cite{cohen1991teamwork} a formal definition of what they call a \textit{persistent goal}:

\begin{quote}
\textbf{Definition: } An agent has a \textit{persistent goal} relative to \textit{q} to achieve \textit{p} iff:
\begin{enumerate}
\item she believes that \textit{p} is currently false;
\item she wants \textit{p} to be true eventually;
\item it is true (and she knows it) that (2) will continue to hold until she comes to believe either that \textit{p} is true, or that it will neither be true, or that \textit{q} is false.
\end{enumerate}
\end{quote}

However, their definition of an \textit{intention} differs a little from the previous one. They define an \textit{intention} as a commitment to act in a certain mental state:

\begin{quote}
\textbf{Definition:} An agent \textit{intends} relative to some conditions to do an action just in case she has a persistent goal (relative to that condition) of having done the action, and, moreover, having done it, believing throughout that she is doing it.
\end{quote}

The \textit{intention} still includes the \textit{goal} but here it concerns more the fact that the agent commits itself to achieve the goal than the way to achieve it.

Let's now apply these principles to a Joint Action. One of the most known definition of \textit{joint intention} is the one of Bratman \cite{bratman1993shared}:

\begin{quote}
We intend to \textit{J} if and only if:
\begin{enumerate}
\item (a) I intend that we \textit{J} and (b) you intend that we \textit{J}.
\item I intend that we \textit{J} in accordance with and because of 1\textit{a}, 1\textit{b}, and meshing subplans of 1\textit{a} and 1\textit{b}; you intend that we \textit{J} in accordance with and because of 1\textit{a}, 1\textit{b}, and meshing subplans of 1\textit{a} and 1\textit{b}.
\item 1 and 2 are common knowledge between us.
\end{enumerate}
\end{quote}

This definition is taking back and illustrated by Tomasello et al. in \cite{tomasello2005understanding} where they reuse the example of the box to open (fig~\ref{fig:intention_jointe}).

\begin{figure}[!h]
	\centering
    \includegraphics[width=0.8\textwidth]{figs/Chapter1/intention_jointe.png}
    \caption{Illustrative example of a collaborative activity by Tomasello et al. Here the humans have for \textit{shared goal} to open the box together. They choose a means to perform it which takes into account the other capabilities and so form a \textit{joint intention}.}
    \label{fig:intention_jointe}
\end{figure}

The \textit{shared goal} is defined as the representation of a desired state plus the fact that it will be done in collaboration with other person(s) (in the example, they will open the box together) and a \textit{joint intention} is defined as a collaborative plan the agents commit to in order to achieve the \textit{shared goal} and which takes into account both agents individual plans (here an agent will hold the box with the clamp while the other open it with the cutter).

In a same way, Cohen and Levesque extend their definition of \textit{persistent goal} and \textit{intention} to a collaborative activity. They first define a \textit{weak achievement goal} as:
\begin{quote}
\textbf{Definition: } An agent has a \textit{weak achievement goal} relative to \textit{q} and with respect to a team to bring about \textit{p} if either of these conditions holds:
\begin{itemize}
\item The agent has a normal achievement goal to bring about \textit{p}, that is, the agent does not yet believe that \textit{p} is true and has \textit{p} eventually being true as goal.
\item The agent believes that \textit{p} is true, will never be true, or is irrelevant (that is, \textit{q} is false), \textit{but} has as a goal that the status of \textit{p} be mutually believed by all the team members.
\end{itemize}
\end{quote}

They then use this definition to define a \textit{joint persistent goal}:
\begin{quote}
\textbf{Definition: } A team of agents have a \textit{joint persistent goal} relative to \textit{q} to achieve \textit{p} just in case
\begin{itemize}
\item they mutually believe that \textit{p} is currently false;
\item they mutually know they all want \textit{p} to eventually be true;
\item it is true (and mutual knowledge) that until they come to mutually believe that \textit{p is true}, that \textit{p} will never be true, or that \textit{q} is false, they will continue to mutually believe that they each have \textit{p} as a weak achievement goal relative to \textit{q} and with respect to the team.
\end{itemize}
\end{quote}

They finally define a \textit{joint intention} as:
\begin{quote}
\textbf{Definition:} A team of agents \textit{jointly intends}, relative to some escape condition, to do an action iff the members have a joint persistent goal relative of that condition of their having done the action and, moreover, having done it mutually believing throughout that they were doing it.
\end{quote}

As previously, the definitions of Cohen and Levesque do no take into account the way to achieve the \textit{shared goal}, however, they introduce the interesting idea that agents are also engaged to communicate about the state of the \textit{shared goal}.

Concerning the way to achieve a \textit{shared goal}, mentioned into the definition of the \textit{joint intention} of Tomasello et al., Grosz and Sidner initially introduce and formalize the notion of \textit{Shared Plan} in \cite{grosz1988plans}, which is extended in \cite{grosz1999evolution}. The key properties of their model are as follows:
\begin{quote}
\begin{enumerate}
\item it uses individual intentions to establish commitment of collaborators to their joint activity
\item it establishes an agent's commitments to its collaborating partners' abilities to carry out their
individual actions that contribute to the joint activity
\item it accounts for helpful behavior in the context of collaborative activity
\item it covers contracting actions and distinguishes contracting from collaboration
\item the need for agents to communicate is derivative, not stipulated, and follows from the general
commitment to the group activity
\item the meshing of subplans is ensured it is also derivative from more general constraints.
\end{enumerate}
\end{quote}

With their definition, each agent does not necessarily know the all \textit{Shared Plan} but only his own individual plan and the meshing subparts of the plan. The group has a \textit{Shared Plan}, but no individual member necessarily has the whole \textit{Shared Plan}.

In conclusion, the concepts concerning the commitment of agents to a collaborative activity that we will use in this thesis can be summarized as:
\begin{itemize}
\item A \textit{goal} will be represented as a desired state.
\item A\textit{shared goal} will be considered as a \textit{goal} to be achieved in collaboration with other partner(s). An agent is considered engaged in a \textit{shared goal} if he believes the goal is currently false, he wants the goal to be true and he will not give up on the goal unless he knows that the goal is achieved, not feasible or not relevant any more and he knows that his partners are aware of it.
\item A \textit{joint intention} will include a \textit{shared goal} and the way to realize it. This way will be represented as a \textit{Shared Plan} which will take into account each agent capacities and the potential conflicts between their actions. This \textit{Shared Plan} will not be necessarily completely known by all members of the group but all individuals will know their part of the plan and the meshing subparts.
\end{itemize}


\subsection{Perception and prediction}

\label{subsec:prediction}

One important thing for an agent when performing a Joint Action is to be able to perceive and predict the actions of his partner and their effects. Based on the works in \cite{sebanz2006joint}, \cite{pacherie2011phenomenology} and \cite{obhi2011moving} we identified several necessary abilities for this predictions:

\paragraph{Joint attention:} The capacity for an agent to direct his attention to where the one of his partner is directed allows to share a representation of objects and events. It brings a better understanding of the other agent knowledge and where his attention is focused and so, it helps the prediction of his possible next actions. Moreover, there should be a mutual manifestation of this joint attention, meaning that we should show that we share the other attention. 

\paragraph{Action observation:} Several studies have shown that when someone observes another person executing an action, a corresponding representation of the action is formed for the observer \cite{rizzolatti2004mirror}. This is done by what has been called the \textit{mirror-neuron} system. This behavior allows the observer to predict the outcomes of the actor's action. 

\paragraph{Co-representation:} An agent needs to have a representation of his partner, including his goal, his capacities and the social rules he is following. Indeed, having this representation will help to predict his future actions. For example, a pedestrian who sees a red traffic light will be able to predict that the car drivers will stop. In a same way, if you know that someone is aiming to go out shopping, you will be able to predict that he will look for the key of the car.


\paragraph{Agency:} Sometimes, when there is a close link between an action performed by oneself and an action performed by another, it can be hard to distinguish who caused a particular action effect. The capacity to attribute the action effects to the good actor is called the sense of \textit{Agency}. This sense of \textit{Agency} is an important thing in Joint Action in order to correctly predict the effects of each action.

\bigskip
Based on the same works as before and on \cite{sebanz2009prediction}, we can list several kinds of predictions to support Joint Action which can be done thanks to the abilities described previously :

\begin{itemize}
\item \textbf{What:} A first one is to predict what will do an agent. Two kinds of predictions can be distinguished here:
\begin{itemize}
\item \textit{action-to-goal:} this is supported by the \textit{mirror-neuron} system introduced before. Here the therm goal concerns the goal of an action, its purpose. The idea is that observing an action, it is possible to predict its goal. For example, if we observe someone extending his arm toward an object we can predict that he will pick the object.
\item \textit{goal-to-action:} here the therm goal concerns the goal of a task, as defined in the previous subsection. Knowing this goal, it can be easy to predict which action an agent will perform.
\end{itemize}
\item \textbf{When:} another prediction which is necessary is the timing of an action. Knowing when an action will occur and during how long allows to a better coordination in time.
\item \textbf{Where:} a Joint Action usually takes place in a shared space. It is therefore necessary to predict the future position of the partner and his actions in order to coordinate in space.
\end{itemize} 


\subsection{Coordination}

\label{subsec:coordination}

The predictions discussed previously allow agents to coordinate during Joint Action. Two kinds of coordination are defined in \cite{knoblich20113} that both support Joint Action.

\paragraph{Emergent coordination:}
It is a coordinated behavior which occurs unintentionally, independently of any joint plans or common knowledge and due to perception-action couplings. Four types of sources of emergent coordination can be distinguished:
\begin{itemize}
\item \textit{Entertainment:} Entrainment is a process that leads to temporal coordination of two actors’ behavior, in particular, synchronization, even in the absence of a direct mechanical coupling. It is the case, for example, for two peoples seating in rocking chairs involuntary synchronizing their rocking frequencies \cite{richardson2007rocking}.
\item \textit{Affordances:} An object affordance represents the opportunities that an object provides to an agent for a certain action repertoire \cite{gibson2014theory}. For example, the different ways to grab a mug. Two kinds of affordances can lead to an emergent coordination: \textit{common affordances} and \textit{joint affordances}. When several agents have the same action repertoire and perceive the same object they have a \textit{common affordance}. This \textit{common affordance} can lead the agents to execute the same action. When an object has affordances for two or more peoples collectively, for an action to occurs agents automatically synchronize. This is what is called \textit{joint affordances}. For example, a long two-handled saw affords cutting for two people acting together but not for either of them acting individually.
\item \textit{Perception-action matching:} As discussed before, observing an action activates corresponding representation in the observer mind. This process can lead to involuntary mimicry the observed action. Consequently, if two persons observe the same action, they can have the same reaction to mimic the action.
\item \textit{Action simulation:} The internal mechanisms activated during action observation not only allow to mimic the action but also to predict the effects of this action. If two people observe the same action and so predict the same effects, they can consequently have the same reaction. For example, two persons seeing the same object fallen will have the same reaction to try to catch it.
\end{itemize}


\paragraph{Planned coordination:} 
While emergent coordination is unintentional, planned coordination requires for agents to plan their own action in relation to Joint Action and others' actions.

One way for an agent to intentionally coordinate during Joint Action is to change his behavior compared to when he is acting alone. These changes of behavior are called \textit{coordination smoother} in \cite{vesper2010minimal} and can be of several types:
\begin{itemize}
\item Making our behavior more predictable by doing for example wider or less variable movements
\item Structuring our own task in order to reduce the need of coordination. For example sharing the space or working turn by turn.
\item Producing coordination signals like looking someone who should act or counting down.
\item Changing the way we use an object by using an affordance more appropriate to a common use.
\end{itemize}

An other way to coordinate is through communication. Indeed, Clark argues that two or more persons can not perform a Joint Action without communicate \cite{clark1996using}. Here the therm communication includes both verbal and non-verbal communication. Clark also defines what he calls the \textit{common ground}: when two agents communicate, they necessarily have common knowledge and conventions. Moreover, when communicating, it is important to not only send a message but also to assure that the message has been understood as the sender intends it to be. This process to make the sender and the receiver mutually believe that the message has been understood well enough for current purposes is called \textit{grounding}.


\section{How to endow a robot with Joint Action abilities}

In this part we will do an overview on how the theory on human-human Joint Action can be applied to human-robot Joint Action. Following to what has been discussed on commitment, we will first see in Sec.~\ref{subsec:engagement} how the robot can engage in Joint Action and understand the intention of its human partners. Then, we will see in Sec.~\ref{subsec:perspective_taking} how the robot perceives the humans and can predict their actions by taking into account their perspectives and mental states. We will also see how the robot can execute actions adapted to Joint Actions with humans in Sec.~\ref{subsec:action} and, finally, we will see how it can coordinate during Joint Action in Sec.~\ref{subsec:coordination_robot}.

The parts which are linked to the work presented in this thesis will be more developed in the corresponding chapters.

\subsection{Engagement and Intention}

\label{subsec:engagement}

As for humans, robots need to be able to engage in Joint Action. A first prerequisite is to choose a goal to perform. This goal can be imposed by a direct order of the user, however, the robot also needs to be able to pro actively propose its help whenever a human needs it. To do so, the robot needs to be able to infer high-level goals by observing and reasoning on its human partners’ activities. This process is called plan recognition or, when a bigger focus is put on Human-Robot Interaction aspects, intention recognition. Many works have been done concerning plan recognition using approaches such as classical planning \cite{ramirez2009plan}, probabilistic planning \cite{bui2003general} or logic-based techniques \cite{singla2011abductive}. Concerning intention recognition, works such as \cite{breazeal2009embodied} and \cite{baker2014modeling} take into account theory of mind aspects to deduce what the human is doing.

When direct orders have been received and humans intentions recognized, the robot needs to choose which goal to perform, also taking into account its own resources. This problem has not been addressed as a whole in the literature, however, some similar works can be seen as partial answer. For example, some deliberation systems allow to solve problems with multiple goals taking into account resources such as time \cite{georgeff1987reactive, ghallab1994representation, lemai2004interleaving} or energy level \cite{rabideau1999iterative}.

Once the robot is engaged in a Joint Action, it needs to be able to monitor other agents engagement. Indeed, it needs to understand if, for a reason, a human aborts the current goal and react accordingly. This can be done using gaze cues and gestures \cite{rich2010recognizing}, postures \cite{sanghvi2011automatic} but also context and humans mental states \cite{salam2015multi}.

Finally, once a goal chosen, the robot needs to be able to establish a Shared Plan to achieve it with its human partner(s). Several works have been done in task planning to takex into account the human \cite{cirillo2010human,Lallement2014hatp}. They allow the robot to reduce resource conflicts \cite{chakraborti2016planning}, take divergent beliefs into account \cite{guitton2012belief,talamadupula2014coordination} or promote stigmergic collaboration
for agents in co-habitation \cite{chakraborti2015planning}. 
Once the plan computed, the robot needs to be able to share/negotiate it with its partners. Several studies have been reported on how to communicate these plans. Some researchers studied how a system could acquire knowledge on plan decomposition from a user \cite{Mohseni2015} and how dialog can be used to teach new collaborative plans to the robot and to modify these plans \cite{petit2013coordinating}. In \cite{sorce2015proof}, the system is able to learn a plan from a user and transmit it to another user and in \cite{allen2002human} a computer agent is able to construct a plan in collaboration with a user. Finally, \cite{milliez2016using} presents a system where the robot shares the plan with a level of details which depends on the expertise of the user.

\subsection{Perspective taking and humans mental states}

\label{subsec:perspective_taking}

\begin{itemize}
\item Need to align robot representation of the world to humans one: x, y, theta to facts.
\item the robot needs to be able to take the perspective of its partner
\item perspective taking + basic usages (dialogue, ...)
\item but robot not only needs to take the point of view of other agents concerning the environment
\item mental states
\end{itemize}

\subsection{Actions realization}

\label{subsec:action}

\begin{itemize}
\item human-aware motion planning
\item human-aware control
\item understanding effects of actions (agency)
\item learning affordances
\end{itemize}


\subsection{Coordination}

\label{subsec:coordination_robot}

\begin{itemize}
\item plan execution
\item dialogue
\item signalling
\item joint actions (e.g. handover)
\end{itemize}

\section{A three levels architecture}

\subsection{The three levels of Pacherie}

\cite{pacherie2008phenomenology}, \cite{pacherie2011phenomenology}

\subsection{A three levels robotics architecture}

+ related work on others robotics architecture


\ifdefined\included
\else
\bibliographystyle{StyleThese}
\bibliography{These}
\end{document}
\fi
