\ifdefined\included
\else
\documentclass[english,a4paper,11pt,twoside]{StyleThese}
\include{formatAndDefs}
\sloppy
\begin{document}
\setcounter{chapter}{2} %% Numéro du chapitre précédent ;)
\dominitoc
\faketableofcontents
\fi

\chapter{Taking Humans Mental States into account while executing Shared Plans}
\minitoc

\label{ch:MS}

\section{Motivations}

When collaborating with humans, it is primordial for the robot to not consider humans as obstacles or tools impacting the environment. As humans are social creatures, the robot must take into account their comfort and so, their point of view. Several works already allow robots to estimate humans perspective and beliefs concerning its environment. In order to improve human-robot Joint Action, the robot must be able to take these information into account when taking decision on how to act or what to communicate. Even if several works have been done on how to integrate humans perspective in dialogue or use it to help the understanding of humans behavior, there is still a gap when it came to use it during Shared Plan execution. This work aims to start filling this gap by extending the robot knowledge on humans mental states to the joint task and using it to better communicate during Shared Plan execution. It has been the subject of a publication into the HRI conference \cite{devin2016implemented}.

\section{Theory of Mind}

\subsection{Social Sciences literature}

Theory of the Mind (ToM) refers to the ability humans have to recognize and attribute mental states not only to themselves but to other people, to understand that feelings and beliefs we have may be different than others and to take others mental states into account. ToM has been deeply studied in psychology, notably in the developmental domain \cite{baron1985does, premack1978does}. \cite{verbrugge2008learning} defines what is called "order" of ToM:
\begin{quote}
"To have a first-order ToM is to assume that someone’s beliefs,
thoughts and desires influence one’s behavior. A first-order thought could be: ‘He does not know that his book is on the table’. In second-order ToM it is also recognized that to predict others’ behavior, the desires and beliefs that they have of one’s self and the predictions of oneself by others must be taken into account. So, for example, you can realize that what someone expects you to do will affect his behavior. For example, ‘(I know) he does not know that I know his book is on the table’ would be part of my second-order ToM. To have a third-order ToM is to assume others to have a second-order ToM, etc."
\end{quote}
There is an infinitesimal numbers of orders, however, studied shown that orders above the second one do not help in cooperative tasks \cite{de2014theory} and above the third one do not help for competitive games \cite{de2014theory}.


\begin{figure*}[!h]
    \centering
    \subfigure[Perceptual perspective taking: two individuals can have a different representation of their environment considering their locations.]{
        \centering
        \includegraphics[width=0.4\textwidth]{figs/Chapter3/perceptual.jpg}
       \label{subfig:perceptual}
   }
    %~
    \subfigure[Conceptual perspective taking: here Bob attributes to Alice a belief concerning the box. He thinks Alice thinks the box is empty.]{
        \centering
        \includegraphics[width=0.4\textwidth]{figs/Chapter3/conceptual.jpg}
       \label{subfig:conceptual}
    }
    \caption{Illustration of perceptual and conceptual perspective taking.}
\end{figure*}

ToM includes the notion of perspective taking: the capacity for a person to reason by taking the point of view of someone else. Studied in literature \cite{tversky1999speakers, flavell1992perspectives}, perspective taking is crucial during humans interaction and studies have demonstrate that individuals who lack of this ability have difficulties in their daily social interactions \cite{frick2014picturing}. Two levels of perspective taking are defined in \cite{flavell1977development}: perceptual and conceptual perspective taking. Perceptual perspective taking design the capacity of a person to understand that others have a different perception of the world (fig~\ref{subfig:perceptual}). Conceptual perspective taking designs the capacity of a person to attribute beliefs and feelings to others (fig~\ref{subfig:conceptual}).

\begin{figure}[!h]
	\centering
    \includegraphics[width=0.7\textwidth]{figs/Chapter3/sally.jpg}
    \caption{The Sally and Anne test: it allows to check the capacity of someone to attribute a false-belief to another person. Illustration from the work of Axel Scheffler.}
    \label{fig:sally}
\end{figure}

To check if an individual has ToM capacities, several tests have been developed in psychology. One of the most famous is the Sally and Anne test (fin~\ref{fig:sally}). This test allows to check the capacity of someone to attribute a false-belief to another person and have been reused in robotics to validate perspective taking systems.

\subsection{Robotics background}

One of the pioneer work in robotics Theory of Mind is \cite{scassellati2002theory}. Scassellati presents two models from social sciences (Leslie \cite{leslie1984spatiotemporal} et Baron-Cohen \cite{baron1997mindblindness}) and proposes a model on how to implement ToM in robotics. However, the implementation of this model did not go further than perception level.

Then, several works have been done in order to endow robots with perspective taking abilities. Using ACT-R architecture \cite{anderson2004integrated}, the team of Hiatt and Trafton models mechanisms used during the Sally and Anne test and constructs a model that learns to deal with false belief to pass this test \cite{hiatt2010cognitive}. They extend this work to second-order in \cite{hiatt2015understanding} and to spatial reasoning in \cite{hiatt2004cognitive}. The Sally and Anne test has also been passed in \cite{milliez2014framework} where the robot constructs a semantic representation of the world from its partners point of view. In \cite{berlin2006perspective}, authors present a way to record different beliefs of other agents and so to have a memory of perspective taking. Finally, \cite{johnson2005perspective} presents a system which computes perspective taking perspective taking based on forward and inverse visual models.

Perspective taking abilities have been used in robotics for several purposes. It has been used in \cite{hiatt2011accommodating} to deal with uncertainty in humans behavior and in \cite{ros2010solving} to solve ambiguous references to an object. One important application of perspective taking is action recognition. \cite{johnson2005perceptual} takes the visual point of view of humans to improve action recognition, Dynamic Bayesian Networks (DBN) are used in \cite{baker2014modeling} or inverse reinforcement learning  in \cite{nagai2015probabilistic}. The human perspective is also used in \cite{breazeal2006using} to learn a task from a situation that can be ambiguous from the robot point of view and in \cite{gray2014manipulating} to choose actions with the adequate effects in order to manipulate humans mental models. Finally, \cite{gorur2017toward} uses perspective taking to infer humans intention and adapt robot decision.

Concerning Shared Plans, they can be found by using perspective taking in order to add communication actions \cite{guitton2012belief}. Then, the human perspective is used to share the plan with a level of details depending of human knowledge \cite{milliez2016using}. However, there is no works for now concerning the management of Shared Plans execution taking into account the human point of view. 

\section{Assumptions}


The work presented in this chapter concerns the estimation of humans knowledge on the task and its use to help the Shared Plan execution. To do so, we make several  assumptions:

\paragraph{Commitment:} we do not focus in this works on issues related to commitment. Consequently, we consider here that the joint goal has already been established. We also consider that none of the humans will abort the goal unless he knows that the goal is not achievable any more.

\paragraph{Shared Plan:} the focus of this work concerns rather the Shared Plan execution than the Shared Plan elaboration. In the examples presented in Sec.~\ref{sec:resultsTOM} the Shared Plan is computed by the robot, however, the process presented in this chapter holds regardless of the way the robot gets the Shared Plan (e.g. it can be imposed by a human or negotiated through dialogue). This chapter will treat only the issues related to ToM usage in Shared Plan execution, the rest of Shared Plan management will be more developed in Chapter \ref{ch:SP}.

\paragraph{ToM order:} this work implements a first-order ToM for the robot (i.e. the robot has knowledge about the human knowledge on the task), the higher orders are not managed for now.

\paragraph{Humans perception:} we make the assumption here that a human will see and understand an action of another agent (mainly robot actions) when he is present and looking at the agent. We
also assume that when he is present, the human is able to hear and understand the information verbalized by the robot.

\paragraph{Robot capacities:} we consider that the robot is able to perform simple high level actions like Pick, Place or Drop. We also assume that the robot is able to ask to a human to perform an action and to inform him about the state of the environment, the goal or an action. The robot is able to detect and localize objects and agents.
and to recognize simple high level actions performed by a human like Pick, Place or Drop. Let us also note that the ways the robot achieves actions (e.g. human-aware motion planning and execution) and recognizes humans’ actions are outside of the scope of this chapter.

\section{Estimating Humans Mental States}

As stated previously, the goal of this work is to fill the gap between existing perspective taking abilities of the robot and Shared Plan execution. A first step to do so is to extend the knowledge of the robot concerning humans mental states to information concerning the Shared Plan. As saw in Chapter~\ref{ch:Sup}, the mental state of a human H will be described as:
$$MS(H) = <g_H, g_R(H), SP(H), WS(H)>$$
where $g_H$ is the goal the robot estimates the human is engaged in, $g_R(H)$ is the goal the robot estimates the human thinks the robot is performing, and $SP(H)$ and $WS(H)$ are the estimation of the Shared Plan and the World State from the human point of view. 

The process to estimate the humans mental states will be noted in the following of the thesis as the operator:
$$MS(H) \leftarrow ESTIMATE\_MS(MS(H), TS)$$

with $TS$ the task state from the robot point of view as stated in Chapter~\ref{ch:Sup}.
We will see now how we estimates each of the mental states components.

\subsection{Goal management ($g_H$ and $g_R(H)$ computation)}

As stated previously, the focus of this work is not put on the goal management. Consequently, the computation of humans mental states concerning goals remains basic. However, a more complex one can be imagined, for example using intention recognition, with the same representation. As a reminder, a goal is defined as:
$$g = <Name_g, Actors_g, Params_g, Obj_g>$$

As we consider humans automatically engaged in the goal, as soon as the robot starts executing a goal, all actors of the goal are considered to have the same goal:
$$ \forall H \in Actors_{g_R}, \ g_H = g_R$$
We also make the basic assumption that all humans who see the robot are aware of its goal:
$$ \forall H  \ | <Robot, isVisibleBy, H> \in WS, \ g_R(H) = g_R$$
For a goal to be considered achieved by an agent (it holds for human mental states as well as robot mental state), this agent needs to have all the objectives of the goal in its knowledge (it means that accordingly to its knowledge, the desired world state has been reached):
$$ \forall H, \ \forall g  \ | \ Obj_g \in WS(H), \ label_g = DONE$$
The robot will consider a goal failed if it does find a plan anymore to achieve it. Concerning the humans, they can be informed through dialogue by the robot of the failure (or success) of a goal.

\subsection{Shared Plan management ($SP(H)$ computation)}

As a reminder, the representation of the Shared Plan $SP$ from a human H point of view is represented as:
$$SP(H) = <id_p(H), A_p(H), L_p(H)>$$
where $id_p(H)$ is used to identify the plan, $A_p(H)$ are the actions composing the plan and $L_p(H)$ the links representing the order the actions should be executed (causal links).

As we consider in this thesis Shared Plans with action allocation evolving during the execution, we made the choice not to share the plan and communicate about actions only when it is not implicit (more details in Chapter~\ref{ch:SP}). Hence, we consider that the Shared Plan of the human is always the same as the robot one, only the state of the actions composing the plan will change:
$$SP(H) = <id_p, A_p(H), L_p>$$

A link $l \in L_p$ can be describe as:
 $$l = \langle prev_l, \ next_l \rangle$$
where $prev_l$ is the id of the action which needs to be achieved before the action with the id $next_l$ is performed. 

The actions composing the plan $A_p(H)$ can be decomposed as:
$$A_p(H) = <A_{prev}(H), A_{cur}(H), A_{next}(H), A_{later}(H)>$$
where $A_{prev(H)}$ are the actions of the plan the human thinks already executed, $A_{cur}(H)$ the actions the human thinks currently executed, $A_{next}(H)$ the actions the human thinks which can be performed and $A_{later}(H)$ the actions the human thinks to be executed in the future. Each action $a$ in $A_{prev}(H)$ is associated with a label noted $label_a$ which can be equal either to DONE, FAILED or ABORTED.

By default, when a Shared Plan is computed by the robot, all actions are put in $A_{later}(H)$. When the robot performs an action or detects an action execution from a human, it considers the human is aware of the action if he can see the actors of the action:

\begin{center}
$a \in A_{cur} \ \& \ (<Ag_a, isVisibleBy, Human> \in WS \ \| \ Human \in Ag_a)$ 

$\Rightarrow a \in A_{cur}(H)$
\end{center}

In a same way, at the end of the execution, the action goes in $A_{prev}(H)$ with the corresponding label if the human performs or saw the actors of the actions at the end of the action.

We also consider that a human can infer that an action has been done if it knows that the action was in progress or on its way to be done and it can see the effects of the action:

\begin{center}
$(a \in A_{cur}(H) \ \| \ a \in A_{next}(H)) \ \& \ Effects_{a} \in WS(H)$ 

$\Rightarrow a \in A_{prev}(H) \ \& \ label_a(H) = DONE$
\end{center}

In a same way, we consider that if a human knows that an action was in progress and can see the actors of the action and that there are not performing the action any more, it considers the action DONE:

\begin{center}
$a \in A_{next}(H) \ \& \ a \in A_{prev} \ \& \ <Ag_a, isVisibleBy, Human> \in WS$

$\Rightarrow a \in A_{prev}(H) \ \& \ label_a(H) = DONE$
\end{center}

Finally, the actions are set in $A_{next}(H)$ considering causal links and preconditions:

\begin{center}
$a \in A_{next}(H) \Leftrightarrow Precs_{a} \in WS(H) \ \& \ (\forall l \in L_p$ 

$| \ next_l = id_a, \exists \ ap \in A_{prev}(H) \ | \ (id_{ap} = prev_l \ \& \ label_{ap}(H)  = DONE))$
\end{center}

The robot can also inform a human about the state of an action, in which case the given information will be add to the human mental state.

\subsection{World State management ($WS(H)$ computation)}
\label{subsec:worldstate}

We saw that the perspective taking abilities of the robot allow it to estimate the human perception of his environment \cite{milliez2014framework}. However, this previous work only concerns information concerning the environment which are perceivable. Indeed, for this work we will consider two kinds of predicates to describe the state of the environment:
\begin{itemize}
\item \textbf{Observable predicates:} they concern what the agent can observe about the world state. These predicates mainly represent the affordances of all agents (e.g. isVisibleBy, isReachableBy) and the relations between objects (e.g. isOn, isIn) visible to them. They are computed continuously by a Situation Assessment module (TOASTER) from the robot and humans point of views based on geometric computations and perspective taking algorithms.
\item \textbf{Non-observable predicates:} they concern information that the agent can not observe (e.g. the fact that an opaque bottle is empty). These predicates are not managed by TOASTER which reason on what is visible by the agents. We consider two ways for an agent to be aware of a non-observable predicate. First, it can perform or see an action that has this predicate in its side effects (if an action $a$ enters into $A_{prev}$ with $label_{a}  = DONE$, $Effects_{a}$ goes into $WS$, likewise with $A_{prev}(H)$ and $WS(H)$). A human can also be aware of a non-observable predicate if it is informed of it by the robot.
\end{itemize}


\section{Mental States for Shared Plans execution}

\begin{itemize}
\item solve db operator
\item weak achievement goal
\item human should act
\item preventing mistakes
\item robot action signalling
\item inaction and uncertainty
\item not attributed action
\end{itemize}

In this paper, we will focus on the activity of the supervisor which allows to manage divergent belief during the execution of a shared plan. Indeed, when two humans share a plan, they usually do not communicate all along the plan execution. Only the \textit{meshing subplans} of the plan need to be shared \cite{bratman1993shared}. Consequently, the robot should inform humans about elements of the shared plan only when it considers that the divergent belief might have an impact on the joint activity in order to not be intrusive by giving them information which they do not need or which they can observe or infer by themselves.


\subsection{Weak achievement goal}

If we follow the definition of \textit{weak achievement goal} in \cite{cohen1991teamwork}, if the robot knows that the current goal has been achieved or is not possible anymore, it has to inform its partners. Accordingly, we consider that, when, in the robot knowledge, the state of a goal is DONE (resp. ABORTED) and the robot estimates that a human does not consider it DONE (resp. ABORTED), the robot informs him about the achievement (resp. abandoning) of the goal (if the agent is not here or is busy with something else, the robot will do it as soon as the agent is available). We extend this reasoning to plans: the robot informs in the same way about the achievement (resp. abandoning) of a plan. 


\subsection{Before human action}

A divergent belief of a human partner can be an issue when it is related to an action that he has to do. To avoid that a human misses information to execute his part of the plan, each time the robot estimates that a human has to do an action (action with the state READY in the robot knowledge and with a human in its actors) it checks if the human is aware that he has to and can perform the action (the state of the same action should be READY in the estimation of the knowledge of the human too). If it is not the case, there are three possibilities:
\begin{itemize}
\item The state of the current plan is not in PROGRESS in the estimation of the agent knowledge: the human misses information about the current plan, so, the robot shares it with him.
\item The state of the action is PLANNED in the estimation of the human knowledge: the human misses information about previous achieved actions to know that his action has to be performed now according to the plan. The robot checks the state of all actions linked to the first one with the plan links and informs about the achievement of all actions with a state different of DONE in the estimation of the human knowledge.
\item The state of the action is NEEDED in the estimation of the human knowledge: the human misses information about the world state to know that his action is possible. In such case, the robot looks into the preconditions of the actions and informs the human about all those the human is not aware of.
\end{itemize}

\subsection{Preventing mistakes}

A divergent belief of a human partner can also be an issue if it leads him to perform an action that is not planned or not to do now according to the plan. To prevent this, for each action that the robot estimates the human thinks READY, the robot checks if the action really needs to be done (action READY in its knowledge too). If it is not the case, the robot corrects the human divergent belief by two different ways:
\begin{itemize}
\item If the action is PLANNED in the robot knowledge: the human thinks that a previous action has been achieved successfully while it is not the case leading him to think he has to perform his action. The robot looks in all actions linked to the first one by the plan links and informs about their state if it is different in the estimation of the human knowledge and in the robot one.
\item If the action is NEEDED in the robot knowledge: the human has a divergent belief concerning the world state that leads him to think that his action is possible while it is not the case. The robot looks into the preconditions of the action and informs about divergent beliefs.
\end{itemize}

\subsection{Signal robot actions}

When the robot has to perform an action, it looks if it estimates that the humans are aware that it will act (the action should be READY in the humans knowledge). If it is not the case, the robot signals its action before performing it.



\subsection{Inaction and uncertainty}

Even if the robot estimates that the human is aware that he has to act (the state of the action which he must perform is READY in the estimation of his knowledge), it is possible that the human still does not perform this action. If the human is already busy by something else (there is an action in the robot knowledge with the state PROGRESS and with the human in its actors), the robot waits for the human to be available. If the human is not considered busy by the robot, the robot first considers that its estimation of the human mental state can be wrong, and that, in reality, the human is not aware that he should act. Consequently, the robot asks the human specifically to do the action. If the human still does not act while the action has been asked, the robot considers the action failed, aborts the current plan and tries to find an alternative plan excluding that action.

In order to avoid considering that the human is available, and so to disturb him while he is busy doing something that the robot can not recognize, we have added an action named \textit{busy} used when the robot estimates that the human is doing something without knowing what. The action $busy$ when executed by a human $h$ can be defined as $\langle id,\ busy,\ \{h\},\ \emptyset,\ \emptyset,\ \emptyset \rangle$.


\section{Results}

\label{sec:resultsTOM}

\subsection{Tasks}

\begin{itemize}
\item clean the table
\item inventory
\end{itemize}

\paragraph{"Clean the table" scenario}


In this example, a PR2 robot and a human have to clean a table together. To do so, they need to remove all items from this table, sweep it, and re-place all previous items. The initial world state is the one in Fig.~\ref{fig:initClean}. We consider that the grey book is reachable only by the robot, the blue book only by the human and the white book by both agents. The actions considered during this task are \textit{pick-and-place} and \textit{sweep}. To pick and place an object on a support, the object and the support need to be reachable by the agent, and, to sweep a surface, it should not have any objects on it and it should be reachable by the agent. The initial plan produced to achieve the goal is shown in  Fig.~\ref{fig:initPlanClean}.

\begin{figure}[!h]
	\centering
    \includegraphics[width=0.7\textwidth]{figs/Chapter3/cleanWithNames.png}
    \caption{Initial situation of the "Clean the table" scenario.}
    \label{fig:initClean}
\end{figure}

\paragraph{"Inventory" scenario}

In this example, a human and a PR2 robot have to make an inventory together. At the beginning of the task, both agents have coloured objects near them as well as a coloured box (initial world state in Fig.~\ref{fig:initInventory}). These coloured objects need to be scanned and then, stored in the box of the same colour. The actions considered during this task are \textit{placeReachable}, \textit{pickanddrop} and \textit{scan}. We call \textit{placeReachable} the action to pick an object and to place it such that it is reachable by the other agent. For an agent to perform this action, the object needs to be reachable by it. We call \textit{pickanddrop} the action to pick an object and to drop it on a box. To perform such action, the object and the box need to be reachable by the agent. The \textit{scan} action can only be performed by the robot and consists of scanning an object by orienting the head of the robot (assumed to be equipped by a scanner) in the direction of the object placed such that it is reachable.  The initial plan produced to achieve the goal is shown in  Fig.~\ref{fig:initPlanInventory}.

\begin{figure}[!h]
	\centering
    \includegraphics[width=0.7\textwidth]{figs/Chapter3/initInventory.png}
    \caption{Initial situation of the "Inventory" scenario.}
    \label{fig:initInventory}
\end{figure}

\begin{figure}[!h]
	\centering
    \includegraphics[width=0.7\textwidth]{figs/Chapter3/planInventory.png}
    \caption{Initial plan of the "Inventory" scenario.}
    \label{fig:initInventory}
\end{figure}


\subsection{Interesting scenario}


To perform the task, the robot computes and verbalizes the plan Fig. \ref{subfig:plan} (with id = 0 and action ids). The robot starts to pick and place the grey book on the light-coloured shelf. The human picks and places the blue book on the dark-coloured shelf then leaves.\footnote{The knowledge presented concerns only plans and actions and only the relevant knowledge has been make explicit here at each step.}


\begin{figure}[!h]
	\centering
    \includegraphics[width=0.7\textwidth]{figs/Chapter3/InitCleanPlan.png}
    \caption{Initial plan of the "Clean the table" scenario.}
    \label{fig:initPlanClean}
\end{figure}

\begin{scriptsize}
\begin{Verbatim}[commandchars=\\\{\}]      
                      ROBOT                                          HUMAN                          
0 planState PROGRESS   3 actionState PLANNED     0 planState PROGRESS   3 actionState PLANNED
0 actionState PROGRESS 4 actionState PLANNED     0 actionState PROGRESS 4 actionState PLANNED
1 actionState DONE     5 actionState PLANNED     1 actionState DONE     5 actionState PLANNED
2 actionState NEEDED   6 actionState PLANNED     2 actionState NEEDED   6 actionState PLANNED  
\end{Verbatim}
\end{scriptsize}


The robot ends its action. At this point, there is no more possible action in the plan because the only action that need to be done now is the one of the human. The robot waits for the human that does not come back and then, aborts the current plan and computes a new one Fig. \ref{fig:newplan} (with id = 1 and action ids).


\begin{figure}[!h]
	\centering
    \includegraphics[width=0.7\textwidth]{figs/Chapter3/SecondCleanPlan.png}
    \caption{Second plan of the "Clean the table" scenario.}
    \label{fig:newplan}
\end{figure}

\begin{scriptsize}
\begin{Verbatim}[commandchars=\\\{\}]              
                      ROBOT                                          HUMAN           
\textcolor{red}{0 planState ABORTED}    \textcolor{red}{8 actionState PLANNED }    0 planState PROGRESS 
\textcolor{red}{0 actionState DONE }    \textcolor{red}{9 actionState PLANNED}     0 actionState PROGRESS 
\textcolor{red}{1 planState PROGRESS}   \textcolor{red}{10 actionState PLANNED}    \textcolor{red}{1 planState UNKNOWN}
\textcolor{red}{7 actionState READY }   \textcolor{red}{11 actionState PLANNED}            
\end{Verbatim}
\end{scriptsize}


The robot picks and places the white book on the light-coloured shelf and cleans the table. The human comes back at this time. As he can see that the grey book is on the shelf near the robot, the robot infers that he infers that the robot has achieved its first action. Moreover, as he can see that the white book is on the shelf near the robot, the robot infers that he infers that the first plan has been aborted since the action to place the white book on the dark-coloured shelf planned in the first plan is not needed any more.

\begin{scriptsize}
\begin{Verbatim}[commandchars=\\\{\}]                                  
                      ROBOT                                          HUMAN                          
0 planState ABORTED    \textcolor{red}{8 actionState DONE}        \textcolor{red}{0 planState ABORTED}
0 actionState DONE     \textcolor{red}{9 actionState READY }      \textcolor{red}{0 actionState DONE}
\textcolor{red}{7 actionState DONE} 
\end{Verbatim}
\end{scriptsize}


The robot assume that the human does not know that he has to place the blue book on the table. The robot notices that he is not aware of the current plan and verbalizes it. The human can observe that the white book is on the shelf near the robot and the robot infers that he infers that the robot did its first action of the new plan.

\begin{scriptsize}
\begin{Verbatim}[commandchars=\\\{\}] 
                      ROBOT                                          HUMAN     
1 planState PROGRESS   9 actionState READY       \textcolor{red}{1 planState PROGRESS}   \textcolor{red}{9 actionState PLANNED}
7 actionState DONE     10 actionState PLANNED    \textcolor{red}{7 actionState DONE}     \textcolor{red}{10 actionState PLANNED}
8 actionState DONE     11 actionState READY      \textcolor{red}{8 actionState READY}    \textcolor{red}{11 actionState PLANNED}
\end{Verbatim}
\end{scriptsize}

However, the robot infers that the human still does not have the knowledge to act as he cannot observe that the table has been cleaned by the robot. Consequently, the robot informs him about this. The human robot will assume now that the human has all the informations to act.\footnote{Available video at \url{http://homepages.laas.fr/sdevin/ICSR2015TOM.mp4}.
}

\begin{scriptsize}
\begin{Verbatim}[commandchars=\\\{\}]   
                      ROBOT                                          HUMAN                         
8 actionState DONE     11 actionState READY      \textcolor{red}{8 actionState DONE}    \textcolor{red}{11 actionState READY}
9 actionState READY                              \textcolor{red}{9 actionState READY}
\end{Verbatim}
\end{scriptsize}

\subsection{Experiment and results}

\begin{itemize}
\item in simulation
\item human follows hatp plan, human leaves
\item criteria
\item results
\item discussion
\end{itemize}


One objective of our contribution is to reduce unnecessary communication acts from the robot during the execution of a shared plan aiming at a more friendly and less intrusive behaviour of the robot. Consequently, in order to evaluate our system, we have chosen to measure the amount of information shared by the robot during a shared plan execution where the human misses some information. As it is not trivial to create and control a lack of knowledge from a human subject, we decided to evaluate our system in simulation. We ran experiments in the two scenarios described previously where a human and a robot have to perform a joint task together\footnote{More details about these two scenarios can be found in the attached video}. When the interaction starts, we consider that the joint goal is already established and that both human and robot already agreed on a shared plan. The robot executes the plan as described in the presented work and the simulated human executes the actions planned for him. We randomly sample a time when the human leaves the scene and another time when the human comes back. While absent, the human does not execute actions and cannot see anything nor communicate.

During the interaction, we logged the number of facts (information chunks) given by the robot to the human. An information concerns either a change in the environment, the state of a previous action, the abortion of a previous plan or the sharing of a plan. We compared our system (called \textit{ToM system}) to:
\begin{itemize}
\item a system which verbalizes any plan abortion, shares any new plan and informs about each action missed by the human (called \textit{Missed system}).
\item a system which verbalizes any plan abortion, shares any new plan and informs about each action performed by the robot even if the human sees it (called \textit{Performed system}).
\end{itemize}

The obtained results are given in Table~\ref{table:results}.

\begin{table}[ht]
\begin{center}
\begin{tabular}{|r||c|c||c|c|}
\hline
 Scenario & \multicolumn{2}{c||}{Clean the table} & \multicolumn{2}{c|}{Inventory}\\
\cline{2-5} 
System & Average & Std Dev & Average & Std Dev\\
\hline
\hline
ToM & 1.32 & 0.98 & 0.41 & 0.48\\
\hline
Missed & 2.86 & 1.33 & 2.61 & 1.36\\
\hline
Performed & 4.44 & 1.85 & 10.0 & 0.0\\
\hline
\end{tabular}
\end{center}
\caption{Number of information given by the robot during the two presented scenarios for the three systems (\textit{TOM}, \textit{Missed} and \textit{Performed}).}
\label{table:results}
\end{table}

We can see that our system allows to reduce significantly the amount of information given by the robot. In the "Clean the table" scenario, depending on when the human leaves, the robot might change the initial plan and take care of the book reachable by both agents instead of the human. This explains why the average number for the \textit{Performed system} is higher than the number of actions initially planned for the robot: the robot performs more actions in the new plan and can communicate about the new plan and the abortion of the previous one. In this scenario, our system allows to communicate about the plan abortion only if the human can not infer it by himself and to not communicate about missed \textit{pickandplace} actions as the human can infer them by looking at the objects placements. However, the robot will inform the human if he missed the fact that the robot has swept the table as it is not observable and it is a necessary information for the human to know before he can put back objects on the table.

In the inventory scenario, as all objects and boxes are reachable only by one agent, the robot does not change the plan when the human leaves. This explains the fact that the standard deviation is null for the \textit{Performed system}: the number of actions performed by the robot never changes and there is no change in the plan. In this scenario, the \textit{pickanddrop} and \textit{scan} actions have non-observable effects (the human can not see an object in a box). However, we can see that our system still verbalizes less information than the \textit{Missed system}: the robot communicates only the information which the human really needs (as the fact that an object the human should drop in a box has been scanned) and does not give information which are not linked to the human part of the plan (as the fact that the robot scanned an object it have to drop in its box).


\section{Conclusion and possible extensions}



\ifdefined\included
\else
\bibliographystyle{StyleThese}
\bibliography{These}
\end{document}
\fi
